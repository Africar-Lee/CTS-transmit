{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining city id: 北京success\n",
      "Get bus data: 1号线\n",
      "地铁1号线八通线(环球度假区-古城) success\n",
      "地铁1号线八通线(古城-环球度假区) success\n",
      "地铁1号线支线(八角游乐园-青龙湖东) success\n",
      "地铁1号线支线(青龙湖东-八角游乐园) success\n",
      "Get bus data: 2号线\n",
      "地铁2号线外环(西直门-西直门) success\n",
      "地铁2号线内环(积水潭-积水潭) success\n",
      "地铁6号线二期(潞城-东小营南) success\n",
      "地铁6号线二期(东小营南-潞城) success\n",
      "Get bus data: 5号线\n",
      "地铁5号线(宋家庄-天通苑北) success\n",
      "地铁5号线(天通苑北-宋家庄) success\n",
      "Get bus data: 6号线\n",
      "地铁6号线(金安桥-潞城) success\n",
      "地铁6号线(潞城-金安桥) success\n",
      "Get bus data: 7号线\n",
      "地铁7号线(北京西站-环球度假区) success\n",
      "地铁7号线(环球度假区-北京西站) success\n",
      "Get bus data: 8号线\n",
      "地铁8号线(朱辛庄-瀛海) success\n",
      "地铁8号线(瀛海-朱辛庄) success\n",
      "Get bus data: 9&房山线\n",
      "地铁9号线(国家图书馆-郭公庄) success\n",
      "地铁9号线(郭公庄-国家图书馆) success\n",
      "Get bus data: 房山线\n",
      "地铁房山线(东管头南-阎村东) success\n",
      "地铁房山线(阎村东-东管头南) success\n",
      "大兴机场大巴房山线(房山区交通局-大兴机场) success\n",
      "大兴机场大巴房山线(大兴机场-房山区交通局) success\n",
      "Get bus data: 10号线\n",
      "地铁10号线外环(车道沟-车道沟) success\n",
      "地铁10号线内环(巴沟-巴沟) success\n",
      "Get bus data: 13号线\n",
      "地铁13号线(东直门-西直门) success\n",
      "地铁13号线(西直门-东直门) success\n",
      "地铁13A号线(天通苑东-车公庄) success\n",
      "地铁13A号线(车公庄-天通苑东) success\n",
      "Get bus data: 15号线\n",
      "地铁15号线(清华东路西口-俸伯) success\n",
      "地铁15号线(俸伯-清华东路西口) success\n",
      "Get bus data: 昌平线\n",
      "地铁昌平线(西土城-昌平西山口) success\n",
      "地铁昌平线(昌平西山口-西土城) success\n",
      "首都机场大巴昌平线(昌平北站-首都机场T3航站楼) success\n",
      "首都机场大巴昌平线(首都机场T3航站楼-昌平北站) success\n",
      "Get bus data: 首都机场线\n",
      "首都机场线(北新桥-2号航站楼) success\n",
      "大兴机场线(草桥-大兴机场) success\n",
      "大兴机场线(大兴机场-草桥) success\n",
      "大兴机场大巴首都机场线(首都机场-大兴机场) success\n",
      "remain_staion_list is  135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/.venv/lib/python3.12/site-packages/transbigdata/crawler.py:284: FutureWarning: You are adding a column named 'geometry' to a GeoDataFrame constructed without an active geometry column. Currently, this automatically sets the active geometry column to 'geometry' but in the future that will no longer happen. Instead, either provide geometry to the GeoDataFrame constructor (GeoDataFrame(... geometry=GeoSeries()) or use `set_geometry('geometry')` to explicitly set the active geometry column.\n",
      "  data['geometry'] = lines\n",
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n",
      "output_df.__len__ is  106848\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from script import dataloader, utility, earlystopping\n",
    "from model import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transbigdata as tbd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# @TODO\n",
    "\n",
    "# station数据预处理函数\n",
    "def station2staion_no_line(station):\n",
    "    station_no_line = pd.DataFrame()\n",
    "    for i in range(len(station['站点名'].unique())):\n",
    "        spec_time_place_station = station[station['站点名'] == station['站点名'].unique()[i]]\n",
    "        # spec_time_place_station = spec_time_place_station[['开始时间','线路名','站点名','进站量','出站量']]\n",
    "        spec_time_place_station = spec_time_place_station.fillna(0)\n",
    "        spec_time_place_station = spec_time_place_station.groupby(['日期','开始时间','结束时间']).agg({\n",
    "            '站点名': 'first',  # 假设每个时间段内站点名是相同的，取第一个即可\n",
    "            '进站量': 'sum',\n",
    "            '出站量': 'sum'\n",
    "        })\n",
    "        spec_time_place_station = spec_time_place_station.reset_index()\n",
    "        station_no_line = pd.concat([station_no_line, spec_time_place_station]).reset_index(drop=True)\n",
    "    return station_no_line\n",
    "\n",
    "\n",
    "# prepare data\n",
    "line,stop = tbd.getbusdata('北京',['1号线', '2号线', '5号线', '6号线', '7号线', '8号线', '9&房山线','房山线', '10号线',\n",
    "       '13号线', '15号线', '昌平线', '首都机场线'])\n",
    "line = line[line['line']!='大兴机场大巴首都机场线'].reset_index(drop=True)\n",
    "stop = stop[stop['line']!='大兴机场大巴首都机场线'].reset_index(drop=True)\n",
    "line = line[line['line']!='首都机场大巴昌平线'].reset_index(drop=True)\n",
    "stop = stop[stop['line']!='首都机场大巴昌平线'].reset_index(drop=True)\n",
    "stop.loc[stop['stationnames'] == '清河站', 'stationnames'] = '清河'\n",
    "\n",
    "line_data = {'线路名': [], '站点名': [], '顺序': []}\n",
    "for i in range(len(stop)):\n",
    "    线路名 = line_data['线路名']\n",
    "    线路名.append(stop.loc[i, 'line'])\n",
    "    站点名 = line_data['站点名']\n",
    "    站点名.append(stop.loc[i, 'stationnames'])\n",
    "    顺序 = line_data['顺序']\n",
    "    顺序.append(stop.loc[i, 'id'])\n",
    "    line_data.update({'线路名':线路名, '站点名':站点名, '顺序':顺序})\n",
    "# 示例数据\n",
    "# line_data = {\n",
    "#     '线路名': ['1号线', '1号线', '1号线', '2号线', '2号线'],\n",
    "#     '站点名': ['站点A', '站点B', '站点C', '站点B', '站点D'],\n",
    "#     '顺序': [1, 2, 3, 1, 2]\n",
    "# }\n",
    "\n",
    "\n",
    "stop_data = {'站点名':list(stop['stationnames'].unique())}\n",
    "# 示例数据\n",
    "# stop_data = {\n",
    "#     '站点名': ['站点A', '站点B', '站点C', '站点D'],\n",
    "#     '其他信息': ['infoA', 'infoB', 'infoC', 'infoD']\n",
    "# }\n",
    "\n",
    "line_for_adj = pd.DataFrame(line_data)\n",
    "stop_for_adj = pd.DataFrame(stop_data)\n",
    "\n",
    "# 构建地铁网络图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加节点\n",
    "for station_for_adj in stop_for_adj['站点名']:\n",
    "    G.add_node(station_for_adj)\n",
    "\n",
    "# 添加边（根据站点顺序）\n",
    "for line_name, group in line_for_adj.groupby('线路名'):\n",
    "    sorted_stations = group.sort_values('顺序')['站点名'].tolist()\n",
    "    for i in range(len(sorted_stations) - 1):\n",
    "        G.add_edge(sorted_stations[i], sorted_stations[i + 1])\n",
    "\n",
    "# 提取邻接矩阵\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# 读取csv\n",
    "path = 'CTS-2024-dataset/'\n",
    "station_csv_file = ['station_20230519.csv',\n",
    "                    'station_20230520.csv',\n",
    "                    'station_20230521.csv',\n",
    "                    'station_20230522.csv',\n",
    "                    'station_20230523.csv',\n",
    "                    'station_20230524.csv',\n",
    "                    'station_20230525.csv',]\n",
    "station_input_csv_select = range(len(station_csv_file))\n",
    "# stationList = utility.cstRawCsvData([path + station_csv_file[i] for i in station_input_csv_select])\n",
    "station = utility.get_station_for_adj(stop_for_adj, [path + station_csv_file[i] for i in station_input_csv_select])\n",
    "station = station2staion_no_line(station) # 合并分布在不同线路上的同一站点数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预加载\n",
    "from datetime import datetime\n",
    "\n",
    "sta_info_for_adj = pd.DataFrame(columns=['station_name', 'abs_date', 'time_point', 'day_of_week', 'is_event', 'in_flow', 'out_flow'])\n",
    "for i in range(len(station)):\n",
    "    station_name = station.loc[i, '站点名']\n",
    "    precise_time = datetime.strptime(station.loc[i, '开始时间'], '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    time_diff = precise_time - datetime.strptime('2023/5/19 00:00:00', '%Y/%m/%d %H:%M:%S')\n",
    "    abs_date = time_diff.days + 1 # 从1开始编号，五月19是第一天\n",
    "\n",
    "    time_point = (precise_time.hour * 60 + precise_time.minute) / 30\n",
    "\n",
    "    day_of_week = precise_time.weekday() # Monday == 0 ... Sunday == 6\n",
    "\n",
    "    event_condition = ((precise_time.day == 26 or precise_time.day == 27) and precise_time.month == 5) or ((precise_time.day == 25) and precise_time.month == 8)\n",
    "    is_event = 1 if event_condition else 0\n",
    "\n",
    "    in_flow = float(station.loc[i, '进站量'])\n",
    "    out_flow = float(station.loc[i, '出站量'])\n",
    "\n",
    "    sta_info_for_adj.loc[i] = [station_name, abs_date, time_point, day_of_week, is_event, in_flow, out_flow]\n",
    "\n",
    "categories_list = list(stop_for_adj['站点名'])\n",
    "my_x = []\n",
    "my_y = []\n",
    "for i in sta_info_for_adj.groupby([\"abs_date\",\"time_point\"]):\n",
    "    temp = i[1]\n",
    "    temp['station_name'] = pd.Categorical(temp['station_name'], categories=categories_list, ordered=True)\n",
    "    temp = temp.sort_values(by=['station_name']).reset_index(drop=True)#特定日期特定时间的所有站点已知数据\n",
    "    features = temp[['abs_date','time_point','day_of_week','is_event']].values\n",
    "    labels = temp[['in_flow','out_flow']].values\n",
    "    my_x.append(features)\n",
    "    my_y.append(labels)\n",
    "\n",
    "my_x = np.array(my_x)\n",
    "my_y = np.array(my_y)\n",
    "\n",
    "\n",
    "# 准备模型输入与loss标杆\n",
    "# 找到所有非零元素的索引\n",
    "adj_matrix = np.array(adj_matrix)\n",
    "row, col = np.nonzero(adj_matrix)\n",
    "\n",
    "# 将这些索引转换为 PyTorch 张量\n",
    "edge_index = torch.tensor(np.array([row, col]), dtype=torch.long)\n",
    "data_input: List[Data] = [Data(x=torch.tensor(my_x[epoch], dtype=torch.float),\n",
    "                               edge_index=edge_index,\n",
    "                               y=torch.tensor(my_y[epoch], dtype=torch.float))\n",
    "                          for epoch in range(len(my_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([285, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input[0].y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model parameters\n",
    "num_features = 4\n",
    "num_classes = 2\n",
    "model = models.GCN(num_features, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0   loss= 4.201837539672852\n",
      "epoch= 50   loss= 11.444269180297852\n",
      "epoch= 100   loss= 7.7712178230285645\n",
      "epoch= 150   loss= 7.287949085235596\n",
      "epoch= 200   loss= 23.9079647064209\n",
      "epoch= 250   loss= 7.97830057144165\n",
      "epoch= 300   loss= 70.45183563232422\n",
      "epoch= 350   loss= 16222.4921875\n",
      "epoch= 400   loss= 105635.7421875\n",
      "epoch= 450   loss= 1337295.25\n",
      "epoch= 500   loss= 324363.375\n",
      "epoch= 550   loss= 56325.03125\n",
      "epoch= 600   loss= 11634.8984375\n",
      "epoch= 650   loss= 28689.958984375\n",
      "epoch= 700   loss= 49130.8125\n",
      "epoch= 750   loss= 10139.6845703125\n",
      "epoch= 800   loss= 18591.54296875\n",
      "epoch= 850   loss= 86377.8828125\n",
      "epoch= 900   loss= 390730.5625\n",
      "epoch= 950   loss= 281291.09375\n",
      "epoch= 1000   loss= 272718.65625\n",
      "epoch= 1050   loss= 234787.6875\n",
      "epoch= 1100   loss= 1274.8477783203125\n",
      "epoch= 1150   loss= 39.222740173339844\n",
      "epoch= 1200   loss= 198.52963256835938\n",
      "epoch= 1250   loss= 5.493224620819092\n",
      "epoch= 1300   loss= 2.284700870513916\n",
      "epoch= 1350   loss= 10.888326644897461\n",
      "epoch= 1400   loss= 1.0088956356048584\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "duptimes = 5 # 暂定每个数据输入进模型训2次\n",
    "total_input_sz = len(data_input)\n",
    "test_set_sz = 48 # 预测集大小\n",
    "train_set_sz = total_input_sz - test_set_sz\n",
    "for epoch in range(duptimes * train_set_sz):\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    out_y = model(data_input[epoch % train_set_sz])  # 前向传播\n",
    "    loss1 = criterion(out_y[0], data_input[epoch % train_set_sz].y[0])  # 计算损失\n",
    "    loss2 = criterion(out_y[1], data_input[epoch % train_set_sz].y[1])\n",
    "    loss = loss1 + loss2  # 加权求和\n",
    "    if epoch%50 == 0:\n",
    "        print('epoch=', epoch, '  loss=', loss.item())\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "\n",
    "# make prediction\n",
    "model.eval()\n",
    "predict_data: List[Data] = data_input[train_set_sz:]\n",
    "\n",
    "predict_in_flow = []\n",
    "predict_out_flow = []\n",
    "for piece in predict_data:\n",
    "    pre_out = model(piece)\n",
    "    predict_in_flow.append(pre_out[:, 0])\n",
    "    predict_out_flow.append(pre_out[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for In-Flow: 188.77220153808594, RMSE for In-Flow: 527.8106079101562\n",
      "MAE for Out-Flow: 188.96250915527344, RMSE for Out-Flow: 558.409423828125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 将预测值和真实值转换为 numpy 数组以便使用 sklearn 库计算评估指标\n",
    "true_in_flow = np.array([data.y[:, 0].detach().numpy() for data in predict_data])\n",
    "true_out_flow = np.array([data.y[:, 1].detach().numpy() for data in predict_data])\n",
    "predicted_in_flow = np.array([out.detach().numpy() for out in predict_in_flow])\n",
    "predicted_out_flow = np.array([out.detach().numpy() for out in predict_out_flow])\n",
    "\n",
    "# 计算 MAE 和 RMSE\n",
    "mae_in = mean_absolute_error(true_in_flow, predicted_in_flow)\n",
    "rmse_in = np.sqrt(mean_squared_error(true_in_flow, predicted_in_flow))\n",
    "\n",
    "mae_out = mean_absolute_error(true_out_flow, predicted_out_flow)\n",
    "rmse_out = np.sqrt(mean_squared_error(true_out_flow, predicted_out_flow))\n",
    "\n",
    "print(f\"MAE for In-Flow: {mae_in}, RMSE for In-Flow: {rmse_in}\")\n",
    "print(f\"MAE for Out-Flow: {mae_out}, RMSE for Out-Flow: {rmse_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
