{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining city id: 北京success\n",
      "Get bus data: 1号线\n",
      "地铁1号线八通线(环球度假区-古城) success\n",
      "地铁1号线八通线(古城-环球度假区) success\n",
      "地铁1号线支线(八角游乐园-青龙湖东) success\n",
      "地铁1号线支线(青龙湖东-八角游乐园) success\n",
      "Get bus data: 2号线\n",
      "地铁2号线外环(西直门-西直门) success\n",
      "地铁2号线内环(积水潭-积水潭) success\n",
      "地铁6号线二期(潞城-东小营南) success\n",
      "地铁6号线二期(东小营南-潞城) success\n",
      "Get bus data: 5号线\n",
      "地铁5号线(宋家庄-天通苑北) success\n",
      "地铁5号线(天通苑北-宋家庄) success\n",
      "Get bus data: 6号线\n",
      "地铁6号线(金安桥-潞城) success\n",
      "地铁6号线(潞城-金安桥) success\n",
      "Get bus data: 7号线\n",
      "地铁7号线(北京西站-环球度假区) success\n",
      "地铁7号线(环球度假区-北京西站) success\n",
      "Get bus data: 8号线\n",
      "地铁8号线(朱辛庄-瀛海) success\n",
      "地铁8号线(瀛海-朱辛庄) success\n",
      "Get bus data: 9&房山线\n",
      "地铁9号线(国家图书馆-郭公庄) success\n",
      "地铁9号线(郭公庄-国家图书馆) success\n",
      "Get bus data: 房山线\n",
      "地铁房山线(东管头南-阎村东) success\n",
      "地铁房山线(阎村东-东管头南) success\n",
      "大兴机场大巴房山线(房山区交通局-大兴机场) success\n",
      "大兴机场大巴房山线(大兴机场-房山区交通局) success\n",
      "Get bus data: 10号线\n",
      "地铁10号线外环(车道沟-车道沟) success\n",
      "地铁10号线内环(巴沟-巴沟) success\n",
      "Get bus data: 13号线\n",
      "地铁13号线(东直门-西直门) success\n",
      "地铁13号线(西直门-东直门) success\n",
      "地铁13A号线(天通苑东-车公庄) success\n",
      "地铁13A号线(车公庄-天通苑东) success\n",
      "Get bus data: 15号线\n",
      "地铁15号线(清华东路西口-俸伯) success\n",
      "地铁15号线(俸伯-清华东路西口) success\n",
      "Get bus data: 昌平线\n",
      "地铁昌平线(西土城-昌平西山口) success\n",
      "地铁昌平线(昌平西山口-西土城) success\n",
      "首都机场大巴昌平线(昌平北站-首都机场T3航站楼) success\n",
      "首都机场大巴昌平线(首都机场T3航站楼-昌平北站) success\n",
      "Get bus data: 首都机场线\n",
      "首都机场线(北新桥-2号航站楼) success\n",
      "大兴机场线(草桥-大兴机场) success\n",
      "大兴机场线(大兴机场-草桥) success\n",
      "大兴机场大巴首都机场线(首都机场-大兴机场) success\n",
      "the length of remain_staion_list is  135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/.venv/lib/python3.12/site-packages/transbigdata/crawler.py:284: FutureWarning: You are adding a column named 'geometry' to a GeoDataFrame constructed without an active geometry column. Currently, this automatically sets the active geometry column to 'geometry' but in the future that will no longer happen. Instead, either provide geometry to the GeoDataFrame constructor (GeoDataFrame(... geometry=GeoSeries()) or use `set_geometry('geometry')` to explicitly set the active geometry column.\n",
      "  data['geometry'] = lines\n",
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/africar/CTS-transmit/script/utility.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  output_df = pd.concat([output_df, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp_rem_df.__len__ is  6480\n",
      "output_df.__len__ is  106848\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from script import dataloader, utility, earlystopping\n",
    "from model import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transbigdata as tbd\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# @TODO\n",
    "\n",
    "# station数据预处理函数\n",
    "def station2staion_no_line(station):\n",
    "    station_no_line = pd.DataFrame()\n",
    "    for i in range(len(station['站点名'].unique())):\n",
    "        spec_time_place_station = station[station['站点名'] == station['站点名'].unique()[i]]\n",
    "        # spec_time_place_station = spec_time_place_station[['开始时间','线路名','站点名','进站量','出站量']]\n",
    "        spec_time_place_station = spec_time_place_station.fillna(0)\n",
    "        spec_time_place_station = spec_time_place_station.groupby(['日期','开始时间','结束时间']).agg({\n",
    "            '站点名': 'first',  # 假设每个时间段内站点名是相同的，取第一个即可\n",
    "            '进站量': 'sum',\n",
    "            '出站量': 'sum'\n",
    "        })\n",
    "        spec_time_place_station = spec_time_place_station.reset_index()\n",
    "        station_no_line = pd.concat([station_no_line, spec_time_place_station]).reset_index(drop=True)\n",
    "    return station_no_line\n",
    "\n",
    "\n",
    "# prepare data\n",
    "line,stop = tbd.getbusdata('北京',['1号线', '2号线', '5号线', '6号线', '7号线', '8号线', '9&房山线','房山线', '10号线',\n",
    "       '13号线', '15号线', '昌平线', '首都机场线'])\n",
    "line = line[line['line']!='大兴机场大巴首都机场线'].reset_index(drop=True)\n",
    "stop = stop[stop['line']!='大兴机场大巴首都机场线'].reset_index(drop=True)\n",
    "line = line[line['line']!='首都机场大巴昌平线'].reset_index(drop=True)\n",
    "stop = stop[stop['line']!='首都机场大巴昌平线'].reset_index(drop=True)\n",
    "stop.loc[stop['stationnames'] == '清河站', 'stationnames'] = '清河'\n",
    "\n",
    "line_data = {'线路名': [], '站点名': [], '顺序': []}\n",
    "for i in range(len(stop)):\n",
    "    线路名 = line_data['线路名']\n",
    "    线路名.append(stop.loc[i, 'line'])\n",
    "    站点名 = line_data['站点名']\n",
    "    站点名.append(stop.loc[i, 'stationnames'])\n",
    "    顺序 = line_data['顺序']\n",
    "    顺序.append(stop.loc[i, 'id'])\n",
    "    line_data.update({'线路名':线路名, '站点名':站点名, '顺序':顺序})\n",
    "# 示例数据\n",
    "# line_data = {\n",
    "#     '线路名': ['1号线', '1号线', '1号线', '2号线', '2号线'],\n",
    "#     '站点名': ['站点A', '站点B', '站点C', '站点B', '站点D'],\n",
    "#     '顺序': [1, 2, 3, 1, 2]\n",
    "# }\n",
    "\n",
    "\n",
    "stop_data = {'站点名':list(stop['stationnames'].unique())}\n",
    "# 示例数据\n",
    "# stop_data = {\n",
    "#     '站点名': ['站点A', '站点B', '站点C', '站点D'],\n",
    "#     '其他信息': ['infoA', 'infoB', 'infoC', 'infoD']\n",
    "# }\n",
    "\n",
    "line_for_adj = pd.DataFrame(line_data)\n",
    "stop_for_adj = pd.DataFrame(stop_data)\n",
    "\n",
    "# 构建地铁网络图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加节点\n",
    "for station_for_adj in stop_for_adj['站点名']:\n",
    "    G.add_node(station_for_adj)\n",
    "\n",
    "# 添加边（根据站点顺序）\n",
    "for line_name, group in line_for_adj.groupby('线路名'):\n",
    "    sorted_stations = group.sort_values('顺序')['站点名'].tolist()\n",
    "    for i in range(len(sorted_stations) - 1):\n",
    "        G.add_edge(sorted_stations[i], sorted_stations[i + 1])\n",
    "\n",
    "# 提取邻接矩阵\n",
    "adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "\n",
    "# 读取csv\n",
    "path = 'CTS-2024-dataset/'\n",
    "station_csv_file = ['station_20230519.csv',\n",
    "                    'station_20230520.csv',\n",
    "                    'station_20230521.csv',\n",
    "                    'station_20230522.csv',\n",
    "                    'station_20230523.csv',\n",
    "                    'station_20230524.csv',\n",
    "                    'station_20230525.csv',]\n",
    "station_input_csv_select = range(len(station_csv_file))\n",
    "station_csv_sel_list = [path + station_csv_file[i] for i in station_input_csv_select]\n",
    "# stationList = utility.cstRawCsvData([path + station_csv_file[i] for i in station_input_csv_select])\n",
    "station = utility.get_station_for_adj(stop_for_adj, station_csv_sel_list)\n",
    "station = station2staion_no_line(station) # 合并分布在不同线路上的同一站点数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预加载\n",
    "from datetime import datetime\n",
    "\n",
    "sta_info_for_adj = pd.DataFrame(columns=['station_name', 'abs_date', 'time_point', 'day_of_week', 'is_event', 'in_flow', 'out_flow'])\n",
    "for i in range(len(station)):\n",
    "    station_name = station.loc[i, '站点名']\n",
    "    precise_time = datetime.strptime(station.loc[i, '开始时间'], '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    time_diff = precise_time - datetime.strptime('2023/5/19 00:00:00', '%Y/%m/%d %H:%M:%S')\n",
    "    abs_date = time_diff.days + 1 # 从1开始编号，五月19是第一天\n",
    "\n",
    "    time_point = (precise_time.hour * 60 + precise_time.minute) / 30\n",
    "\n",
    "    day_of_week = precise_time.weekday() # Monday == 0 ... Sunday == 6\n",
    "\n",
    "    event_condition = ((precise_time.day == 26 or precise_time.day == 27) and precise_time.month == 5) or ((precise_time.day == 25) and precise_time.month == 8)\n",
    "    is_event = 1 if event_condition else 0\n",
    "\n",
    "    in_flow = float(station.loc[i, '进站量'])\n",
    "    out_flow = float(station.loc[i, '出站量'])\n",
    "\n",
    "    sta_info_for_adj.loc[i] = [station_name, abs_date, time_point, day_of_week, is_event, in_flow, out_flow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of remain_staion_list is  135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False,  True, False,  True,  True, False,\n",
       "        False,  True, False,  True,  True, False,  True, False,  True,  True,\n",
       "         True,  True, False,  True,  True,  True, False, False,  True,  True,\n",
       "         True,  True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True, False, False, False,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "         True, False, False,  True,  True, False, False, False,  True, False,\n",
       "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
       "         True,  True, False, False, False, False,  True, False, False, False,\n",
       "        False,  True, False, False,  True,  True,  True, False, False,  True,\n",
       "         True, False,  True,  True, False, False, False,  True,  True, False,\n",
       "         True,  True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "         True,  True, False, False,  True, False, False, False,  True, False,\n",
       "         True, False, False,  True, False, False, False,  True, False, False,\n",
       "        False,  True,  True, False,  True,  True,  True, False,  True,  True,\n",
       "        False,  True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        False, False,  True,  True,  True, False, False,  True,  True, False,\n",
       "         True, False, False,  True,  True, False, False, False, False,  True,\n",
       "         True,  True, False,  True,  True, False,  True,  True,  True,  True,\n",
       "         True, False,  True, False, False, False, False, False,  True,  True,\n",
       "        False, False,  True, False,  True, False,  True,  True,  True, False,\n",
       "         True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        False, False, False,  True,  True,  True, False, False,  True, False,\n",
       "         True, False, False, False,  True,  True, False,  True, False,  True,\n",
       "         True, False, False,  True, False, False, False, False, False, False,\n",
       "         True, False, False,  True,  True, False, False,  True,  True,  True,\n",
       "         True, False,  True, False,  True,  True, False, False,  True,  True,\n",
       "        False,  True, False,  True,  True, False,  True,  True,  True, False,\n",
       "         True,  True, False, False, False])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_station_mask(stop_for_adj: pd.DataFrame, csv_list: list[str]) -> torch.tensor:\n",
    "    output_mask = torch.ones(len(stop_for_adj)).tolist()\n",
    "    categories_list = list(stop_for_adj['站点名'])\n",
    "    rem_stop_list = utility.get_rem_sta_list(stop_for_adj, csv_list)\n",
    "    for stop_i in range(len(categories_list)):\n",
    "        if categories_list[stop_i] in rem_stop_list:\n",
    "            output_mask[stop_i] = False\n",
    "\n",
    "    for i in range(len(output_mask)):\n",
    "        if output_mask[i]:\n",
    "            output_mask[i] = True\n",
    "\n",
    "    return torch.tensor(output_mask)\n",
    "\n",
    "station_mask = get_station_mask(stop_for_adj, station_csv_sel_list)\n",
    "station_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categories_list = list(stop_for_adj['站点名'])\n",
    "scaler = StandardScaler()\n",
    "my_x = []\n",
    "my_y = []\n",
    "for i in sta_info_for_adj.groupby([\"abs_date\",\"time_point\"]):\n",
    "    temp = i[1]\n",
    "    temp['station_name'] = pd.Categorical(temp['station_name'], categories=categories_list, ordered=True)\n",
    "    temp = temp.sort_values(by=['station_name']).reset_index(drop=True)#特定日期特定时间的所有站点已知数据\n",
    "    features = temp[['abs_date','time_point','day_of_week','is_event']].values\n",
    "    labels = temp[['in_flow','out_flow']].values\n",
    "    features = scaler.fit_transform(features)\n",
    "    my_x.append(features)\n",
    "    my_y.append(labels)\n",
    "\n",
    "my_x = np.array(my_x)\n",
    "my_y = np.array(my_y)\n",
    "\n",
    "\n",
    "\n",
    "# 准备模型输入与loss标杆\n",
    "# 找到所有非零元素的索引\n",
    "adj_matrix = np.array(adj_matrix)\n",
    "row, col = np.nonzero(adj_matrix)\n",
    "\n",
    "# 将这些索引转换为 PyTorch 张量\n",
    "edge_index = torch.tensor(np.array([row, col]), dtype=torch.long)\n",
    "data_input: List[Data] = [Data(x=torch.tensor(my_x[epoch], dtype=torch.float),\n",
    "                               edge_index=edge_index,\n",
    "                               y=torch.tensor(my_y[epoch], dtype=torch.float),\n",
    "                               node_mask=station_mask)\n",
    "                          for epoch in range(len(my_x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model parameters\n",
    "num_features = 4\n",
    "num_classes = 2\n",
    "model = models.GCN(num_features, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'node_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 清除梯度\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_set_sz):\n\u001b[0;32m---> 10\u001b[0m     out_y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     out_y1_for_loss \u001b[38;5;241m=\u001b[39m out_y[:, \u001b[38;5;241m0\u001b[39m][station_mask]\n\u001b[1;32m     12\u001b[0m     out_y2_for_loss \u001b[38;5;241m=\u001b[39m out_y[:, \u001b[38;5;241m1\u001b[39m][station_mask]\n",
      "File \u001b[0;32m~/CTS-transmit/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTS-transmit/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/CTS-transmit/model/models.py:14\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m     13\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m---> 14\u001b[0m     node_mask \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_mask\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         x \u001b[38;5;241m=\u001b[39m x[node_mask]\n",
      "File \u001b[0;32m~/CTS-transmit/.venv/lib/python3.12/site-packages/torch_geometric/data/data.py:559\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CTS-transmit/.venv/lib/python3.12/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'node_mask'"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "duptimes = 200 # 每个数据输入进模型训练的次数\n",
    "total_input_sz = len(data_input)\n",
    "test_set_sz = 48 # 预测集大小\n",
    "train_set_sz = total_input_sz - test_set_sz\n",
    "model.train()\n",
    "for dup_i in range(duptimes):\n",
    "    optimizer.zero_grad()  # 清除梯度\n",
    "    for epoch in range(train_set_sz):\n",
    "        out_y = model(data_input[epoch])  # 前向传播\n",
    "        out_y1_for_loss = out_y[:, 0][station_mask]\n",
    "        out_y2_for_loss = out_y[:, 1][station_mask]\n",
    "        true_y1_for_loss = data_input[epoch].y[:, 0][station_mask]\n",
    "        true_y2_for_loss = data_input[epoch].y[:, 1][station_mask]\n",
    "        loss1 = criterion(out_y1_for_loss, true_y1_for_loss)  # 计算损失\n",
    "        loss2 = criterion(out_y2_for_loss, true_y2_for_loss)\n",
    "    loss = loss1 + loss2  # 加权求和\n",
    "    if dup_i % (duptimes / 20) == 0:\n",
    "        print('dup_i=', dup_i, '\\t\\tloss=', loss.item())\n",
    "    loss.backward()  # 反向传播\n",
    "    optimizer.step()  # 更新参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "model.eval()\n",
    "predict_data: List[Data] = data_input[train_set_sz:]\n",
    "\n",
    "predict_in_flow = []\n",
    "predict_out_flow = []\n",
    "for piece in predict_data:\n",
    "    pre_out = model(piece)\n",
    "    predict_in_flow.append(pre_out[:, 0][station_mask])\n",
    "    predict_out_flow.append(pre_out[:, 1][station_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for In-Flow: 359.47381591796875, RMSE for In-Flow: 728.1807250976562\n",
      "MAE for Out-Flow: 357.5871887207031, RMSE for Out-Flow: 769.400634765625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 将预测值和真实值转换为 numpy 数组以便使用 sklearn 库计算评估指标\n",
    "true_in_flow = np.array([data.y[:, 0][station_mask].detach().numpy() for data in predict_data])\n",
    "true_out_flow = np.array([data.y[:, 1][station_mask].detach().numpy() for data in predict_data])\n",
    "predicted_in_flow = np.array([out.detach().numpy() for out in predict_in_flow])\n",
    "predicted_out_flow = np.array([out.detach().numpy() for out in predict_out_flow])\n",
    "\n",
    "# 计算 MAE 和 RMSE\n",
    "mae_in = mean_absolute_error(true_in_flow, predicted_in_flow)\n",
    "rmse_in = np.sqrt(mean_squared_error(true_in_flow, predicted_in_flow))\n",
    "\n",
    "mae_out = mean_absolute_error(true_out_flow, predicted_out_flow)\n",
    "rmse_out = np.sqrt(mean_squared_error(true_out_flow, predicted_out_flow))\n",
    "\n",
    "print(f\"MAE for In-Flow: {mae_in}, RMSE for In-Flow: {rmse_in}\")\n",
    "print(f\"MAE for Out-Flow: {mae_out}, RMSE for Out-Flow: {rmse_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
